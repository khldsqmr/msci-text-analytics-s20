Report your classification accuracy results in a table with three different activation functions in the hidden layer(ReLU, sigmoid and tanh). What effect do activation functions have on your results? What effect does addition of L2-norm regularization have on the results? What effect does dropout have on the results?Explain your intuitions briefly


Activation functions:

ReLU, Accuracy = 
Sigmoid, Accuracy = 80.74%
Tanh, Accuracy = 

Using different non-linear activation functions (ReLU, Sigmoid and tanh), we observe 

  <table>
    <thead>
      <tr>
        <th>"Activation functions"</th>
        <th>"Accuracy"</th>
      </tr>
    </thead>
    <tbody>
        <tr>
            <td>"ReLU"</td>
            <td>80.71%</td>
        </tr>
        <tr>
            <td>"Sigmoid"</td>
            <td>80.75%</td>
        </tr>
	<tr>
            <td>"Tanh"</td>
            <td>80.54%</td>
        </tr>
    </tbody>
  </table>
